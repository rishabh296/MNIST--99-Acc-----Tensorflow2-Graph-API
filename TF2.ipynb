{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"/tmp/mylogs/tf/\"\n",
    "\n",
    "#Visualize Plots - % tensorboard --logdir=/tmp/mylogs/tf/\n",
    "\n",
    "classes = 10\n",
    "\n",
    "# Training parameters.\n",
    "lr = 0.001\n",
    "batch_size = 100\n",
    "display_step = 600\n",
    "epochs = 50\n",
    "\n",
    "# Network parameters.\n",
    "conv1_filters = 32 \n",
    "conv2_filters = 64 \n",
    "fc1_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MNIST data.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "# Normalization\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat(epochs).shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers weight & bias parameters\n",
    "random_normal = tf.initializers.RandomNormal()\n",
    " \n",
    "weights = {\n",
    "    'wc1': tf.Variable(random_normal([5, 5, 1, conv1_filters])),\n",
    "    'wc2': tf.Variable(random_normal([5, 5, conv1_filters, conv2_filters])),\n",
    "    'wd1': tf.Variable(random_normal([7*7*64, fc1_units])),\n",
    "    'out': tf.Variable(random_normal([fc1_units, classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros([conv1_filters])),\n",
    "    'bc2': tf.Variable(tf.zeros([conv2_filters])),\n",
    "    'bd1': tf.Variable(tf.zeros([fc1_units])),\n",
    "    'out': tf.Variable(tf.zeros([classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "@tf.function\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Layer CNN Model\n",
    "@tf.function\n",
    "def model(x,weights,biases):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    with tf.name_scope('CONV1'):\n",
    "        conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "        conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    with tf.name_scope('CONV2'):\n",
    "        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "        conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    with tf.name_scope('FLATTEN'):\n",
    "        flatten = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    \n",
    "    with tf.name_scope('FC'):\n",
    "        fc = tf.add(tf.matmul(flatten, weights['wd1']), biases['bd1'])\n",
    "        fc = tf.nn.relu(fc)\n",
    "\n",
    "    with tf.name_scope('Output'):\n",
    "        output = tf.add(tf.matmul(fc, weights['out']), biases['out'])\n",
    "        return tf.nn.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy loss function\n",
    "@tf.function\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    y_true = tf.one_hot(y_true, depth=classes)\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
    "    return loss\n",
    "\n",
    "# Accuracy metric\n",
    "@tf.function\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "def optimization(x, y,optimizer,weights,biases):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = model(x,weights,biases)\n",
    "        loss = cross_entropy(pred, y)\n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return pred,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 3.301328, train accuracy: 0.980000, val accuracy: 0.986100, val loss: 437.595276\n",
      "epoch: 2, train loss: 1.751071, train accuracy: 0.990000, val accuracy: 0.991800, val loss: 249.643219\n",
      "epoch: 3, train loss: 4.362897, train accuracy: 0.980000, val accuracy: 0.991600, val loss: 261.813507\n",
      "epoch: 4, train loss: 0.666313, train accuracy: 1.000000, val accuracy: 0.990400, val loss: 301.683350\n",
      "epoch: 5, train loss: 0.313856, train accuracy: 1.000000, val accuracy: 0.992800, val loss: 257.396851\n",
      "epoch: 6, train loss: 0.178863, train accuracy: 1.000000, val accuracy: 0.990900, val loss: 303.299042\n",
      "epoch: 7, train loss: 0.492146, train accuracy: 1.000000, val accuracy: 0.991000, val loss: 306.120605\n",
      "epoch: 8, train loss: 0.016612, train accuracy: 1.000000, val accuracy: 0.991500, val loss: 327.495117\n",
      "epoch: 9, train loss: 0.009616, train accuracy: 1.000000, val accuracy: 0.991700, val loss: 334.207703\n",
      "epoch: 10, train loss: 0.341440, train accuracy: 1.000000, val accuracy: 0.992800, val loss: 297.861328\n",
      "epoch: 11, train loss: 1.461700, train accuracy: 0.990000, val accuracy: 0.992100, val loss: 314.437744\n",
      "epoch: 12, train loss: 0.051565, train accuracy: 1.000000, val accuracy: 0.992500, val loss: 296.982574\n",
      "epoch: 13, train loss: 0.005820, train accuracy: 1.000000, val accuracy: 0.993500, val loss: 296.829712\n",
      "epoch: 14, train loss: 2.673669, train accuracy: 0.990000, val accuracy: 0.989900, val loss: 540.703003\n",
      "epoch: 15, train loss: 0.001831, train accuracy: 1.000000, val accuracy: 0.991400, val loss: 417.720245\n",
      "epoch: 16, train loss: 0.001329, train accuracy: 1.000000, val accuracy: 0.992200, val loss: 364.117249\n",
      "epoch: 17, train loss: 0.001116, train accuracy: 1.000000, val accuracy: 0.991500, val loss: 393.215912\n",
      "epoch: 18, train loss: 0.007847, train accuracy: 1.000000, val accuracy: 0.989100, val loss: 574.482666\n",
      "epoch: 19, train loss: 0.037605, train accuracy: 1.000000, val accuracy: 0.992900, val loss: 343.076538\n",
      "epoch: 20, train loss: 0.003162, train accuracy: 1.000000, val accuracy: 0.991500, val loss: 419.865295\n",
      "Total Training Time -  0:28:58.937694\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "logdir_adam = logdir + \"adam/\"+ now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.create_file_writer(logdir_adam)\n",
    "\n",
    "# ADAM optimizer.\n",
    "optimizer = tf.optimizers.Adam(lr)\n",
    "\n",
    "# Run training for the given number of steps.\n",
    "with writer.as_default():\n",
    "    for step, (batch_x, batch_y) in enumerate(train_data, 1):\n",
    "            \n",
    "        pred,loss = optimization(batch_x, batch_y,optimizer,weights,biases)\n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            acc = accuracy(pred, batch_y)\n",
    "            val_pred = model(x_test,weights,biases)\n",
    "            val_acc = accuracy(val_pred, y_test)\n",
    "            val_loss = cross_entropy(val_pred,y_test)\n",
    "\n",
    "            tf.summary.scalar('train accuracy', acc, step = step//display_step)\n",
    "            tf.summary.scalar('training loss', loss, step=step//display_step)\n",
    "            tf.summary.scalar('test accuracy', val_acc, step = step//display_step)\n",
    "            tf.summary.scalar('test loss', val_loss, step=step//display_step)\n",
    "            print(\"epoch: %i, train loss: %f, train accuracy: %f, val accuracy: %f, val loss: %f\" % (step/display_step, loss, acc, val_acc, val_loss))\n",
    "            writer.flush()\n",
    "\n",
    "print('Total Training Time ADAM - ', datetime.now()-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinitializing weights for SGD optimizer\n",
    "weights['wc1'].assign(random_normal([5, 5, 1, conv1_filters]))\n",
    "weights['wc2'].assign(random_normal([5, 5, conv1_filters, conv2_filters]))\n",
    "weights['wd1'].assign(random_normal([7*7*64, fc1_units]))\n",
    "weights['out'].assign(random_normal([fc1_units, classes]))\n",
    "\n",
    "biases['bc1'].assign(tf.zeros([conv1_filters]))\n",
    "biases['bc2'].assign(tf.zeros([conv2_filters]))\n",
    "biases['bd1'].assign(tf.zeros([fc1_units]))\n",
    "biases['out'].assign(tf.zeros([classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 4.479516, train accuracy: 0.980000, val accuracy: 0.979400, val loss: 644.624634\n",
      "epoch: 2, train loss: 11.314090, train accuracy: 0.970000, val accuracy: 0.986800, val loss: 413.193970\n",
      "epoch: 3, train loss: 1.648815, train accuracy: 0.990000, val accuracy: 0.989300, val loss: 322.847076\n",
      "epoch: 4, train loss: 0.279901, train accuracy: 1.000000, val accuracy: 0.989400, val loss: 286.581268\n",
      "epoch: 5, train loss: 8.757600, train accuracy: 0.990000, val accuracy: 0.989400, val loss: 289.366638\n",
      "epoch: 6, train loss: 0.370599, train accuracy: 1.000000, val accuracy: 0.988300, val loss: 373.910370\n",
      "epoch: 7, train loss: 0.876777, train accuracy: 1.000000, val accuracy: 0.991100, val loss: 303.402588\n",
      "epoch: 8, train loss: 0.637351, train accuracy: 1.000000, val accuracy: 0.990500, val loss: 271.908752\n",
      "epoch: 9, train loss: 1.178825, train accuracy: 0.990000, val accuracy: 0.991100, val loss: 259.002533\n",
      "epoch: 10, train loss: 0.145763, train accuracy: 1.000000, val accuracy: 0.991500, val loss: 257.209747\n",
      "epoch: 11, train loss: 0.031776, train accuracy: 1.000000, val accuracy: 0.991200, val loss: 260.925232\n",
      "epoch: 12, train loss: 0.158652, train accuracy: 1.000000, val accuracy: 0.991700, val loss: 275.346558\n",
      "epoch: 13, train loss: 0.065120, train accuracy: 1.000000, val accuracy: 0.991500, val loss: 285.123047\n",
      "epoch: 14, train loss: 0.020743, train accuracy: 1.000000, val accuracy: 0.991300, val loss: 279.786926\n",
      "epoch: 15, train loss: 0.008547, train accuracy: 1.000000, val accuracy: 0.991100, val loss: 281.511841\n",
      "epoch: 16, train loss: 0.110586, train accuracy: 1.000000, val accuracy: 0.990100, val loss: 304.544434\n",
      "epoch: 17, train loss: 0.048114, train accuracy: 1.000000, val accuracy: 0.991200, val loss: 304.124512\n",
      "epoch: 18, train loss: 0.033198, train accuracy: 1.000000, val accuracy: 0.990700, val loss: 304.487061\n",
      "epoch: 19, train loss: 0.011981, train accuracy: 1.000000, val accuracy: 0.990700, val loss: 311.657135\n",
      "epoch: 20, train loss: 0.010168, train accuracy: 1.000000, val accuracy: 0.991200, val loss: 320.705627\n",
      "Total Training Time SGD -  0:10:49.680104\n"
     ]
    }
   ],
   "source": [
    "# SGD optimizer.\n",
    "optimizer = tf.optimizers.SGD(lr)\n",
    "\n",
    "now = datetime.now()\n",
    "logdir_sgd = logdir + \"sgd/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.create_file_writer(logdir_sgd)\n",
    "\n",
    "# Training Batches\n",
    "with writer.as_default():\n",
    "    for step, (batch_x, batch_y) in enumerate(train_data, 1):\n",
    "        \n",
    "        pred,loss = optimization(batch_x, batch_y,optimizer,weights,biases)\n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            acc = accuracy(pred, batch_y)\n",
    "            val_pred = model(x_test,weights,biases)\n",
    "            val_acc = accuracy(val_pred, y_test)\n",
    "            val_loss = cross_entropy(val_pred,y_test)\n",
    "\n",
    "            tf.summary.scalar('train accuracy', acc, step = step//display_step)\n",
    "            tf.summary.scalar('training loss', loss, step=step//display_step)\n",
    "            tf.summary.scalar('test accuracy', val_acc, step = step//display_step)\n",
    "            tf.summary.scalar('test loss', val_loss, step=step//display_step)\n",
    "            print(\"epoch: %i, train loss: %f, train accuracy: %f, val accuracy: %f, val loss: %f\" % (step/display_step, loss, acc, val_acc, val_loss))\n",
    "            writer.flush()\n",
    "\n",
    "print('Total Training Time SGD - ', datetime.now()-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
